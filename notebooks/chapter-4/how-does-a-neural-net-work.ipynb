{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a function with gradient descent\n",
    "\n",
    "A neural network is just a mathematical function. In the most standard kind of neural network, the function:\n",
    "\n",
    "Multiplies each input by a number of values. These values are known as parameters\n",
    "\n",
    "- Adds them up for each group of values\n",
    "- Replaces the negative numbers with zeros\n",
    "- This represents one \"layer\". Then these three steps are repeated, using the outputs of the previous layer as the inputs to the next layer. Initially, the parameters in this function are selected randomly. Therefore a newly created neural network doesn't do anything useful at all -- it's just random!\n",
    "\n",
    "To get the function to \"learn\" to do something useful, we have to change the parameters to make them \"better\" in some way. We do this using gradient descent. Let's see how this works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from fastai.basics import *\n",
    "\n",
    "\n",
    "plt.rc(\"figure\", dpi=90)\n",
    "\n",
    "\n",
    "def plot_function(f, title=None, min=-2.1, max=2.1, color=\"r\", ylim=None):\n",
    "  x = torch.linspace(min, max, 100)[:,None]\n",
    "  if ylim:\n",
    "    plt.ylim(ylim)\n",
    "  plt.plot(x, f(x), color)\n",
    "  if title:\n",
    "    plt.title(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
